{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voronoi-based Void Detection: MLP vs GNN\n",
    "\n",
    "This notebook demonstrates how to use Voronoi tessellation features to detect cosmic voids while avoiding spatial data leakage.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Spatial Data Leakage**: When models memorize galaxy positions rather than learning void characteristics\n",
    "2. **Voronoi Features**: Topological properties (volume, neighbor count) that avoid leakage\n",
    "3. **MLP vs GNN**: Comparing simple feature-based vs graph-based approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import VoidX modules\n",
    "from voidx.voronoi import VoronoiFeatureExtractor\n",
    "from voidx.models import VoidMLP\n",
    "from voidx.data import GalaxyDataset, split_indices_stratified, normalize_features\n",
    "\n",
    "# Check if GNN is available\n",
    "try:\n",
    "    from voidx.models import VoronoiGCN, VoronoiGAT\n",
    "    from torch_geometric.data import Data\n",
    "    GNN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GNN_AVAILABLE = False\n",
    "    print(\"torch_geometric not installed. GNN models will not be available.\")\n",
    "    print(\"Install with: pip install torch-geometric\")\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load galaxy positions and void labels. If you have prepared data from `data_preparation_synthetic.ipynb`, load it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load existing data\n",
    "data_path = Path('data/knn_data.npz')\n",
    "\n",
    "if data_path.exists():\n",
    "    print(f\"Loading data from {data_path}...\")\n",
    "    data = np.load(data_path)\n",
    "    positions = data['positions']\n",
    "    labels = data['membership'].astype(np.float32)\n",
    "    print(f\"Loaded {len(positions)} galaxies\")\n",
    "    print(f\"Void fraction: {labels.mean():.2%}\")\n",
    "else:\n",
    "    print(\"No existing data found. Generating synthetic data...\")\n",
    "    # Generate simple synthetic data\n",
    "    n_galaxies = 5000\n",
    "    box_size = 250.0\n",
    "    \n",
    "    # Background galaxies (not in voids)\n",
    "    n_bg = int(n_galaxies * 0.7)\n",
    "    positions_bg = np.random.uniform(0, box_size, size=(n_bg, 3))\n",
    "    labels_bg = np.zeros(n_bg)\n",
    "    \n",
    "    # Void galaxies\n",
    "    n_void = n_galaxies - n_bg\n",
    "    positions_void = np.random.uniform(0, box_size, size=(n_void, 3))\n",
    "    labels_void = np.ones(n_void)\n",
    "    \n",
    "    positions = np.vstack([positions_bg, positions_void])\n",
    "    labels = np.hstack([labels_bg, labels_void]).astype(np.float32)\n",
    "    \n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(n_galaxies)\n",
    "    positions = positions[idx]\n",
    "    labels = labels[idx]\n",
    "    \n",
    "    print(f\"Generated {n_galaxies} galaxies ({labels.mean():.2%} in voids)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Voronoi Features\n",
    "\n",
    "Extract Voronoi tessellation features: cell volumes, neighbor counts, and adjacency graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Voronoi feature extractor\n",
    "box_size = 250.0  # Adjust based on your data\n",
    "extractor = VoronoiFeatureExtractor(\n",
    "    box_size=box_size,\n",
    "    use_periodic=True,\n",
    "    clip_infinite=True,\n",
    ")\n",
    "\n",
    "print(\"Computing Voronoi tessellation...\")\n",
    "features = extractor.extract_features(positions)\n",
    "\n",
    "print(f\"\\nVoronoi statistics:\")\n",
    "print(f\"  Number of cells: {len(positions)}\")\n",
    "print(f\"  Number of edges: {features['edge_index'].shape[1]}\")\n",
    "print(f\"  Avg neighbors per cell: {features['neighbor_count'].mean():.2f}\")\n",
    "print(f\"  Median cell volume: {np.nanmedian(features['volumes']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Features\n",
    "\n",
    "Let's look at the distribution of volumes and neighbor counts for void vs non-void galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Volume distribution\n",
    "void_mask = labels == 1\n",
    "nonvoid_mask = labels == 0\n",
    "\n",
    "volumes = features['volumes']\n",
    "valid_mask = ~np.isnan(volumes)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(np.log10(volumes[valid_mask & nonvoid_mask]), bins=50, alpha=0.5, label='Non-void', density=True)\n",
    "ax.hist(np.log10(volumes[valid_mask & void_mask]), bins=50, alpha=0.5, label='Void', density=True)\n",
    "ax.set_xlabel('Log10(Cell Volume)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Cell Volume Distribution')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Neighbor count distribution\n",
    "ax = axes[1]\n",
    "ax.hist(features['neighbor_count'][nonvoid_mask], bins=30, alpha=0.5, label='Non-void', density=True)\n",
    "ax.hist(features['neighbor_count'][void_mask], bins=30, alpha=0.5, label='Void', density=True)\n",
    "ax.set_xlabel('Number of Neighbors')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Neighbor Count Distribution')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(f\"Void cells - Mean log(volume): {np.log10(volumes[valid_mask & void_mask]).mean():.2f}\")\n",
    "print(f\"Non-void cells - Mean log(volume): {np.log10(volumes[valid_mask & nonvoid_mask]).mean():.2f}\")\n",
    "print(f\"Void cells - Mean neighbors: {features['neighbor_count'][void_mask].mean():.2f}\")\n",
    "print(f\"Non-void cells - Mean neighbors: {features['neighbor_count'][nonvoid_mask].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Training\n",
    "\n",
    "Split data into train/validation/test sets using stratified splitting to preserve class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_idx, val_idx, test_idx = split_indices_stratified(\n",
    "    labels, train=0.7, val=0.15, seed=42\n",
    ")\n",
    "\n",
    "print(f\"Data split:\")\n",
    "print(f\"  Train: {len(train_idx)} samples ({labels[train_idx].mean():.2%} void)\")\n",
    "print(f\"  Val:   {len(val_idx)} samples ({labels[val_idx].mean():.2%} void)\")\n",
    "print(f\"  Test:  {len(test_idx)} samples ({labels[test_idx].mean():.2%} void)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train MLP Model\n",
    "\n",
    "Train an MLP using only topological features (volume + neighbor count) to avoid spatial leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP features (NO positions!)\n",
    "mlp_features = extractor.create_mlp_features(positions, include_positions=False)\n",
    "\n",
    "X_train = mlp_features[train_idx]\n",
    "X_val = mlp_features[val_idx]\n",
    "X_test = mlp_features[test_idx]\n",
    "\n",
    "# Normalize\n",
    "X_train, X_val, X_test, _ = normalize_features(X_train, X_val, X_test)\n",
    "\n",
    "y_train = labels[train_idx]\n",
    "y_val = labels[val_idx]\n",
    "y_test = labels[test_idx]\n",
    "\n",
    "print(f\"MLP features shape: {mlp_features.shape}\")\n",
    "print(f\"Features: normalized_volume, neighbor_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and loaders\n",
    "train_dataset = GalaxyDataset(X_train, y_train)\n",
    "val_dataset = GalaxyDataset(X_val, y_val)\n",
    "test_dataset = GalaxyDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Create model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "mlp_model = VoidMLP(\n",
    "    in_features=X_train.shape[1],\n",
    "    hidden_layers=(128, 64, 32),\n",
    "    dropout=0.3,\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in mlp_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"Training MLP...\")\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    mlp_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = mlp_model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    mlp_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            pred = mlp_model(X_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            predicted = (pred > 0.5).float()\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = mlp_model.state_dict().copy()\n",
    "\n",
    "# Load best model\n",
    "mlp_model.load_state_dict(best_model_state)\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(train_losses, label='Train')\n",
    "ax.plot(val_losses, label='Validation')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training History - Loss')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(val_accs)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Validation Accuracy')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "mlp_model.eval()\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        pred = mlp_model(X_batch)\n",
    "        test_preds.append(pred.cpu().numpy())\n",
    "        test_labels.append(y_batch.numpy())\n",
    "\n",
    "test_preds = np.concatenate(test_preds)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "test_pred_labels = (test_preds > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = (test_pred_labels == test_labels).mean()\n",
    "tp = ((test_pred_labels == 1) & (test_labels == 1)).sum()\n",
    "fp = ((test_pred_labels == 1) & (test_labels == 0)).sum()\n",
    "fn = ((test_pred_labels == 0) & (test_labels == 1)).sum()\n",
    "tn = ((test_pred_labels == 0) & (test_labels == 0)).sum()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\nMLP Test Results:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train GNN Model (Optional)\n",
    "\n",
    "If torch_geometric is installed, train a GNN model that leverages the graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GNN_AVAILABLE:\n",
    "    # Create GNN features (include positions in graph context)\n",
    "    gnn_features = np.hstack([\n",
    "        features['normalized_volumes'][:, np.newaxis],\n",
    "        features['neighbor_count'][:, np.newaxis],\n",
    "        positions,  # OK in GNN due to graph context\n",
    "    ])\n",
    "    \n",
    "    # Normalize\n",
    "    X_train_gnn = gnn_features[train_idx]\n",
    "    X_val_gnn = gnn_features[val_idx]\n",
    "    X_test_gnn = gnn_features[test_idx]\n",
    "    X_train_gnn, X_val_gnn, X_test_gnn, (mean, std) = normalize_features(\n",
    "        X_train_gnn, X_val_gnn, X_test_gnn\n",
    "    )\n",
    "    \n",
    "    # Normalize full dataset\n",
    "    gnn_features_norm = (gnn_features - mean) / std\n",
    "    \n",
    "    # Create masks\n",
    "    train_mask = np.zeros(len(labels), dtype=bool)\n",
    "    val_mask = np.zeros(len(labels), dtype=bool)\n",
    "    test_mask = np.zeros(len(labels), dtype=bool)\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    print(f\"GNN features shape: {gnn_features.shape}\")\n",
    "    print(f\"Features: normalized_volume, neighbor_count, x, y, z\")\n",
    "    print(f\"Note: Positions used in graph context\")\n",
    "else:\n",
    "    print(\"Skipping GNN training - torch_geometric not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GNN_AVAILABLE:\n",
    "    # Create graph data\n",
    "    data = Data(\n",
    "        x=torch.as_tensor(gnn_features_norm, dtype=torch.float32),\n",
    "        edge_index=torch.as_tensor(features['edge_index'], dtype=torch.long),\n",
    "        y=torch.as_tensor(labels, dtype=torch.float32),\n",
    "    ).to(device)\n",
    "    \n",
    "    train_mask_t = torch.as_tensor(train_mask, dtype=torch.bool).to(device)\n",
    "    val_mask_t = torch.as_tensor(val_mask, dtype=torch.bool).to(device)\n",
    "    test_mask_t = torch.as_tensor(test_mask, dtype=torch.bool).to(device)\n",
    "    \n",
    "    # Create model\n",
    "    gnn_model = VoronoiGCN(\n",
    "        in_features=gnn_features.shape[1],\n",
    "        hidden_channels=64,\n",
    "        num_layers=3,\n",
    "        dropout=0.3,\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"\\nGNN model parameters: {sum(p.numel() for p in gnn_model.parameters()):,}\")\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer_gnn = torch.optim.Adam(gnn_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    criterion_gnn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    epochs_gnn = 200\n",
    "    best_val_loss_gnn = float('inf')\n",
    "    train_losses_gnn = []\n",
    "    val_losses_gnn = []\n",
    "    val_accs_gnn = []\n",
    "    \n",
    "    print(\"\\nTraining GNN...\")\n",
    "    for epoch in range(epochs_gnn):\n",
    "        # Training\n",
    "        gnn_model.train()\n",
    "        optimizer_gnn.zero_grad()\n",
    "        \n",
    "        out = gnn_model(data.x, data.edge_index)\n",
    "        loss = criterion_gnn(out[train_mask_t], data.y[train_mask_t])\n",
    "        loss.backward()\n",
    "        optimizer_gnn.step()\n",
    "        \n",
    "        train_losses_gnn.append(loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        gnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = gnn_model(data.x, data.edge_index)\n",
    "            val_loss = criterion_gnn(out[val_mask_t], data.y[val_mask_t]).item()\n",
    "            \n",
    "            pred = torch.sigmoid(out[val_mask_t]) > 0.5\n",
    "            val_acc = (pred == data.y[val_mask_t]).float().mean().item()\n",
    "        \n",
    "        val_losses_gnn.append(val_loss)\n",
    "        val_accs_gnn.append(val_acc)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}: Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss_gnn:\n",
    "            best_val_loss_gnn = val_loss\n",
    "            best_model_state_gnn = gnn_model.state_dict().copy()\n",
    "    \n",
    "    # Load best model\n",
    "    gnn_model.load_state_dict(best_model_state_gnn)\n",
    "    print(\"\\nGNN Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GNN_AVAILABLE:\n",
    "    # Plot GNN training history\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax.plot(train_losses_gnn, label='Train', alpha=0.7)\n",
    "    ax.plot(val_losses_gnn, label='Validation')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('GNN Training History - Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax = axes[1]\n",
    "    ax.plot(val_accs_gnn)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('GNN Validation Accuracy')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate GNN on test set\n",
    "    gnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = gnn_model(data.x, data.edge_index)\n",
    "        test_preds_gnn = torch.sigmoid(out[test_mask_t]).cpu().numpy()\n",
    "        test_labels_gnn = data.y[test_mask_t].cpu().numpy()\n",
    "    \n",
    "    test_pred_labels_gnn = (test_preds_gnn > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_gnn = (test_pred_labels_gnn == test_labels_gnn).mean()\n",
    "    tp_gnn = ((test_pred_labels_gnn == 1) & (test_labels_gnn == 1)).sum()\n",
    "    fp_gnn = ((test_pred_labels_gnn == 1) & (test_labels_gnn == 0)).sum()\n",
    "    fn_gnn = ((test_pred_labels_gnn == 0) & (test_labels_gnn == 1)).sum()\n",
    "    tn_gnn = ((test_pred_labels_gnn == 0) & (test_labels_gnn == 0)).sum()\n",
    "    \n",
    "    precision_gnn = tp_gnn / (tp_gnn + fp_gnn) if (tp_gnn + fp_gnn) > 0 else 0\n",
    "    recall_gnn = tp_gnn / (tp_gnn + fn_gnn) if (tp_gnn + fn_gnn) > 0 else 0\n",
    "    f1_gnn = 2 * precision_gnn * recall_gnn / (precision_gnn + recall_gnn) if (precision_gnn + recall_gnn) > 0 else 0\n",
    "    \n",
    "    print(\"\\nGNN Test Results:\")\n",
    "    print(f\"  Accuracy:  {accuracy_gnn:.4f}\")\n",
    "    print(f\"  Precision: {precision_gnn:.4f}\")\n",
    "    print(f\"  Recall:    {recall_gnn:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1_gnn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Results\n",
    "\n",
    "Compare the performance of MLP and GNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<15} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1':>10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'MLP':<15} {accuracy:>10.4f} {precision:>10.4f} {recall:>10.4f} {f1:>10.4f}\")\n",
    "\n",
    "if GNN_AVAILABLE:\n",
    "    print(f\"{'GNN (GCN)':<15} {accuracy_gnn:>10.4f} {precision_gnn:>10.4f} {recall_gnn:>10.4f} {f1_gnn:>10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Key Takeaways:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. MLP uses only topological features (volume, neighbor count)\")\n",
    "print(\"   - Avoids spatial data leakage\")\n",
    "print(\"   - Good baseline performance\")\n",
    "print(\"\")\n",
    "if GNN_AVAILABLE:\n",
    "    print(\"2. GNN leverages graph structure + node features\")\n",
    "    print(\"   - Can capture multi-scale patterns\")\n",
    "    print(\"   - Uses positions within graph context\")\n",
    "    print(\"   - Better performance on complex structures\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Test spatial generalization**: Split data spatially (e.g., by box regions) to verify no leakage\n",
    "2. **Try other GNN models**: VoronoiGAT (attention-based) or VoronoiSAGE\n",
    "3. **Feature engineering**: Add more Voronoi features (cell shape, anisotropy, etc.)\n",
    "4. **Hyperparameter tuning**: Optimize model architecture and training parameters\n",
    "5. **Apply to real data**: Test on observational catalogs\n",
    "\n",
    "For more details, see `VORONOI_GNN_GUIDE.md` and `examples/compare_mlp_gnn_voronoi.py`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
